# ComfyUI AI Assistant

ä»Šå¤©å‘ç°é˜¿é‡Œå·²ç»æœ‰ä¸€ä¸ªåŒæ ·çš„é¡¹ç›®ï¼šhttps://github.com/AIDC-AI/ComfyUI-Copilot/tree
å¤§å…¬å¸åšçš„äº§å“è‚¯å®šæ•ˆæœå¥½ï¼Œæˆ‘ä¸ªäººåšèµ·æ¥æ•ˆç‡ä¸è¡Œï¼Œæ•ˆæœä¹Ÿè¾¾ä¸åˆ°è¦æ±‚ï¼Œä»¥åè¿™ä¸ªé¡¹ç›®å°†ä¸åœ¨æ›´æ–°

ComfyUI AI Assistant æ˜¯ä¸€ä¸ªé›†æˆäº ComfyUI çš„ AI åŠ©æ‰‹æ‰©å±•ï¼Œæ”¯æŒå¤šç§ AI æœåŠ¡ï¼ŒåŒ…æ‹¬æœ¬åœ°æ¨¡å‹ã€äº‘ç«¯ API ä»¥åŠå…è´¹å¼€æºæ¨¡å‹æœåŠ¡ã€‚åŠ©æ‰‹å¯ä»¥å›ç­”é—®é¢˜ã€ååŠ©å·¥ä½œæµåˆ›å»ºä»¥åŠå¤„ç†å›¾åƒç­‰ä»»åŠ¡ã€‚

## åŠŸèƒ½ç‰¹ç‚¹

- ğŸ¤– æ”¯æŒå¤šç§ AI æœåŠ¡ï¼ˆG4Fã€æœ¬åœ°æ¨¡å‹ç­‰ï¼‰
- ğŸ–¼ï¸ æ”¯æŒå›¾åƒä¸Šä¼ å’Œå¤„ç†
- ğŸ’¬ æµå¼å“åº”ï¼Œå®æ—¶æ˜¾ç¤º AI å›å¤
- ğŸ“ ä¿å­˜å’ŒåŠ è½½èŠå¤©å†å²
- ğŸ”§ å¯è‡ªå®šä¹‰çš„å‚æ•°è®¾ç½®
- ğŸŒ æ— éœ€å¤–éƒ¨ API å¯†é’¥

## å®‰è£…æ–¹æ³•

### æ–¹æ³•ä¸€ï¼šGit Cloneï¼ˆæ¨èï¼‰

```bash
cd custom_nodes
git clone https://github.com/panqinjian/comfy_ai_assistant.git
cd comfy_ai_assistant
pip install -r requirements.txt
```

### æ–¹æ³•äºŒï¼šä¸‹è½½ Zip

1. ä¸‹è½½æœ¬ä»“åº“çš„ ZIP æ–‡ä»¶
2. è§£å‹åˆ° ComfyUI çš„ `custom_nodes` ç›®å½•
3. è¿›å…¥è§£å‹åçš„ç›®å½•ï¼Œè¿è¡Œ `pip install -r requirements.txt`

## ä¾èµ–é¡¹

è¯¦è§ `requirements.txt` æ–‡ä»¶ã€‚ä¸»è¦ä¾èµ–åŒ…æ‹¬ï¼š

- aiohttp - å¼‚æ­¥ HTTP å®¢æˆ·ç«¯/æœåŠ¡å™¨
- g4f - å…è´¹ GPT API æœåŠ¡
- pillow - å›¾åƒå¤„ç†
- python-dotenv - ç¯å¢ƒå˜é‡ç®¡ç†
- requests - HTTP è¯·æ±‚
- ujson - é«˜æ€§èƒ½ JSON å¤„ç†

## ä½¿ç”¨æ–¹æ³•

1. å¯åŠ¨ ComfyUI
2. ç‚¹å‡»ç•Œé¢å³ä¸‹è§’çš„ AI åŠ©æ‰‹å›¾æ ‡
3. åœ¨è®¾ç½®ä¸­é€‰æ‹© AI æœåŠ¡å’Œæ¨¡å‹
4. å¼€å§‹å¯¹è¯

## å¼€å‘æ–‡æ¡£

### ç›®å½•ç»“æ„

```
comfy_ai_assistant/
â”œâ”€â”€ __init__.py               # å…¥å£æ–‡ä»¶
â”œâ”€â”€ requirements.txt          # ä¾èµ–æ¸…å•
â”œâ”€â”€ models_cache/             # æ¨¡å‹ç¼“å­˜ç›®å½•
â”œâ”€â”€ ai_services/              # AI æœåŠ¡å®ç°
â”‚   â”œâ”€â”€ __init__.py           # æœåŠ¡æ¨¡å—åˆå§‹åŒ–
â”‚   â”œâ”€â”€ base_service.py       # åŸºç¡€æœåŠ¡ç±»
â”‚   â”œâ”€â”€ service_manager.py    # æœåŠ¡ç®¡ç†å™¨
â”‚   â””â”€â”€ integrations/         # å…·ä½“æœåŠ¡å®ç°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ g4f_service.py    # G4F æœåŠ¡å®ç°
â”‚       â”œâ”€â”€ local_service.py  # æœ¬åœ°æ¨¡å‹æœåŠ¡
â”‚       â””â”€â”€ claude_service.py # Claude æœåŠ¡
â”œâ”€â”€ server/                   # æœåŠ¡å™¨å®ç°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api_server.py         # API æœåŠ¡å™¨
â”‚   â”œâ”€â”€ routes.py             # è·¯ç”±å®šä¹‰
â”‚   â”œâ”€â”€ utils.py              # æœåŠ¡å™¨å·¥å…·
â”‚   â””â”€â”€ validators.py         # æ•°æ®éªŒè¯
â””â”€â”€ web/                      # å‰ç«¯å®ç°
    â”œâ”€â”€ ai_index.js           # å…¥å£æ–‡ä»¶
    â”œâ”€â”€ ai_ui.js              # UI ç®¡ç†
    â”œâ”€â”€ manifest.json         # æ‰©å±•æ¸…å•
    â”œâ”€â”€ README.md             # å‰ç«¯æ–‡æ¡£
    â”œâ”€â”€ styles/               # æ ·å¼
    â”œâ”€â”€ services/             # å‰ç«¯æœåŠ¡
    â””â”€â”€ components/           # UI ç»„ä»¶
```

### åç«¯æ¶æ„

ComfyUI AI Assistant åç«¯é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œä¸»è¦ç”±ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ç»„æˆï¼š

#### 1. å…¥å£æ¨¡å— (`__init__.py`)

è´Ÿè´£åˆå§‹åŒ–æ‰©å±•å¹¶æ³¨å†Œåˆ° ComfyUIï¼ŒåŒ…æ‹¬ï¼š

- åˆå§‹åŒ– API æœåŠ¡å™¨
- æ³¨å†Œæ‰©å±• Web ç›®å½•
- é…ç½®æ—¥å¿—ç³»ç»Ÿ
- æœåŠ¡æ¨¡å‹åˆå§‹åŒ–

#### 2. æœåŠ¡ç®¡ç† (`ai_services/`)

AI æœåŠ¡å±‚è´Ÿè´£ä¸å„ç§ AI æ¨¡å‹çš„äº¤äº’ï¼š

- `base_service.py`: å®šä¹‰æœåŠ¡åŸºç±»æ¥å£
- `service_manager.py`: ç®¡ç†æ‰€æœ‰ AI æœåŠ¡å®ä¾‹
- `integrations/`: å„ç§ AI æœåŠ¡çš„å…·ä½“å®ç°

#### 3. æœåŠ¡å™¨ (`server/`)

å¤„ç† HTTP è¯·æ±‚å’Œè·¯ç”±ï¼š

- `api_server.py`: API æœåŠ¡å™¨å®ç°
- `routes.py`: è·¯ç”±å®šä¹‰
- `utils.py`: å·¥å…·å‡½æ•°
- `validators.py`: è¯·æ±‚æ•°æ®éªŒè¯

### æ ¸å¿ƒç±»å’Œæ¥å£

#### 1. æœåŠ¡åŸºç±» (`BaseService`)

```python
class BaseService:
    def __init__(self, service_id, name, description, **kwargs):
        self.service_id = service_id
        self.name = name
        self.description = description
        self.is_initialized = False
        self.config = {}
        self.set_config(kwargs)
    
    def set_config(self, config):
        """è®¾ç½®æœåŠ¡é…ç½®"""
        self.config.update(config)
    
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡ï¼Œå­ç±»éœ€å®ç°"""
        self.is_initialized = True
        return True
    
    async def send_message(self, message, history=None, options=None):
        """å‘é€æ¶ˆæ¯ï¼Œå­ç±»éœ€å®ç°"""
        raise NotImplementedError
    
    def get_models(self):
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼Œå­ç±»éœ€å®ç°"""
        raise NotImplementedError
    
    def get_config_template(self):
        """è·å–é…ç½®æ¨¡æ¿ï¼Œå­ç±»éœ€å®ç°"""
        raise NotImplementedError
```

#### 2. æœåŠ¡ç®¡ç†å™¨ (`ServiceManager`)

```python
class ServiceManager:
    def __init__(self):
        self.services = {}
        self.default_service = None
    
    def register_service(self, service):
        """æ³¨å†Œ AI æœåŠ¡"""
        self.services[service.service_id] = service
        if self.default_service is None:
            self.default_service = service.service_id
    
    def get_service(self, service_id=None):
        """è·å–æŒ‡å®š ID çš„æœåŠ¡å®ä¾‹"""
        if service_id is None:
            service_id = self.default_service
        return self.services.get(service_id)
    
    def get_all_services(self):
        """è·å–æ‰€æœ‰å·²æ³¨å†ŒæœåŠ¡"""
        return {id: {
            "id": s.service_id,
            "name": s.name,
            "description": s.description
        } for id, s in self.services.items()}
```

#### 3. G4F æœåŠ¡å®ç° (`G4FService`)

```python
class G4FService(BaseService):
    def __init__(self):
        super().__init__(
            service_id="g4f",
            name="G4F Models",
            description="Free and open source models via G4F library"
        )
        self.cache_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "models_cache")
        self.cache_file = os.path.join(self.cache_dir, "g4f_models.json")
        os.makedirs(self.cache_dir, exist_ok=True)
    
    def get_models(self):
        """è·å–æ¨¡å‹åˆ—è¡¨ï¼Œä¼˜å…ˆä»ç½‘ç»œè·å–ï¼Œå¤±è´¥åˆ™ä»ç¼“å­˜è¯»å–ï¼Œéƒ½å¤±è´¥åˆ™ä½¿ç”¨é™æ€åˆ—è¡¨"""
        try:
            # å°è¯•ä» G4F åŠ¨æ€è·å–
            models = []
            # è·å–æ‰€æœ‰æ¨¡å‹
            for model in g4f.models.ModelUtils.convert():
                model_id = model.__name__
                model_name = model.__name__.replace('_', '-').upper()
                model_path = f"g4f.models.{model_id}"
                models.append({
                    "id": model_id,
                    "name": model_name,
                    "model_path": model_path
                })
            
            # æˆåŠŸè·å–åä¿å­˜åˆ°ç¼“å­˜
            self._save_to_cache(models)
            return models
        except Exception as e:
            logger.warning(f"Failed to get models from G4F: {e}")
            
            # å°è¯•ä»ç¼“å­˜è¯»å–
            try:
                return self._load_from_cache()
            except Exception as e:
                logger.warning(f"Failed to load models from cache: {e}")
                
                # ä½¿ç”¨é™æ€åˆ—è¡¨ä½œä¸ºå¤‡é€‰
                static_models = [
                    {"id": "gpt_35_turbo", "name": "GPT-3.5-Turbo", "model_path": "g4f.models.gpt_35_turbo"},
                    {"id": "gpt_4", "name": "GPT-4", "model_path": "g4f.models.gpt_4"},
                    {"id": "claude", "name": "Claude", "model_path": "g4f.models.claude"},
                ]
                
                # ä¿å­˜é™æ€åˆ—è¡¨åˆ°ç¼“å­˜
                self._save_to_cache(static_models)
                return static_models
    
    def _map_model_name(self, model_name):
        """æ˜ å°„æ¨¡å‹åç§°åˆ°å®é™…æ¨¡å‹å¯¹è±¡"""
        models = self.get_models()
        
        # æ£€æŸ¥æ˜¯å¦ç›´æ¥åŒ¹é… ID æˆ–åç§°
        for model in models:
            if model_name.lower() == model['id'].lower() or model_name.lower() == model['name'].lower():
                try:
                    # ä½¿ç”¨ model_path åŠ¨æ€è·å–æ¨¡å‹å¯¹è±¡
                    model_path = model['model_path'].split('.')
                    model_module = importlib.import_module('.'.join(model_path[:-1]))
                    return getattr(model_module, model_path[-1])
                except (ImportError, AttributeError) as e:
                    logger.error(f"Error importing model {model_name}: {e}")
                    break
        
        # é»˜è®¤è¿”å›GPT-3.5
        return g4f.models.gpt_35_turbo

    async def send_message(self, message, history=None, options=None):
        """å‘é€æ¶ˆæ¯åˆ° G4F æœåŠ¡"""
        if options is None:
            options = {}
        
        model_name = options.get('model', 'gpt_35_turbo')
        model = self._map_model_name(model_name)
        
        # æ„å»ºæ¶ˆæ¯å†å²
        messages = []
        if history:
            for h in history:
                messages.append({"role": h["role"], "content": h["content"]})
        
        # æ·»åŠ å½“å‰æ¶ˆæ¯
        if isinstance(message, str):
            messages.append({"role": "user", "content": message})
        else:
            messages.append(message)
        
        try:
            # è°ƒç”¨ G4F API
            response = await g4f.ChatCompletion.create_async(
                model=model,
                messages=messages,
                temperature=float(options.get('temperature', 0.7)),
                top_p=float(options.get('top_p', 1.0)),
                max_tokens=int(options.get('max_tokens', 4000)),
                stream=options.get('stream', False)
            )
            
            return response
        except Exception as e:
            logger.error(f"Error in G4F service: {e}")
            return f"Error: {str(e)}"
    
    def _save_to_cache(self, models):
        """ä¿å­˜æ¨¡å‹åˆ—è¡¨åˆ°ç¼“å­˜æ–‡ä»¶"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(models, f, indent=2)
        except Exception as e:
            logger.error(f"Failed to save models to cache: {e}")
    
    def _load_from_cache(self):
        """ä»ç¼“å­˜æ–‡ä»¶åŠ è½½æ¨¡å‹åˆ—è¡¨"""
        if not os.path.exists(self.cache_file):
            raise FileNotFoundError(f"Cache file not found: {self.cache_file}")
        
        with open(self.cache_file, 'r', encoding='utf-8') as f:
            return json.load(f)
```

#### 4. API æœåŠ¡å™¨ (`APIServer`)

```python
class APIServer:
    def __init__(self, comfy_app):
        self.comfy_app = comfy_app
        self.service_manager = ServiceManager()
        self.initialize_services()
        self.register_routes()
    
    def initialize_services(self):
        """åˆå§‹åŒ–å¹¶æ³¨å†Œæ‰€æœ‰ AI æœåŠ¡"""
        # æ³¨å†Œ G4F æœåŠ¡
        g4f_service = G4FService()
        self.service_manager.register_service(g4f_service)
        
        # æ³¨å†Œå…¶ä»–æœåŠ¡...
    
    def register_routes(self):
        """æ³¨å†Œ API è·¯ç”±"""
        from .routes import register_routes
        register_routes(self.comfy_app, self.service_manager)
```

### API è·¯ç”±

API è·¯ç”±å®šä¹‰åœ¨ `server/routes.py` ä¸­ï¼Œä¸»è¦åŒ…æ‹¬ï¼š

#### 1. è·å–æœåŠ¡åˆ—è¡¨

```python
@routes.get("/ai/services")
async def get_services(request):
    """è·å–æ‰€æœ‰å¯ç”¨çš„ AI æœåŠ¡"""
    service_manager = request.app['ai_service_manager']
    services = service_manager.get_all_services()
    return web.json_response({"success": True, "services": services})
```

#### 2. è·å–æ¨¡å‹åˆ—è¡¨

```python
@routes.get("/ai/models")
async def get_models(request):
    """è·å–æŒ‡å®šæœåŠ¡çš„å¯ç”¨æ¨¡å‹"""
    service_manager = request.app['ai_service_manager']
    service_id = request.query.get('service', None)
    
    service = service_manager.get_service(service_id)
    if not service:
        return web.json_response({
            "success": False,
            "error": f"Service {service_id} not found"
        }, status=404)
    
    models = service.get_models()
    return web.json_response({"success": True, "models": models})
```

#### 3. å‘é€æ¶ˆæ¯

```python
@routes.post("/ai/chat")
async def chat(request):
    """å‘é€æ¶ˆæ¯åˆ° AI æœåŠ¡"""
    try:
        data = await request.json()
        service_manager = request.app['ai_service_manager']
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        validate_chat_request(data)
        
        # è·å–æœåŠ¡
        service_id = data.get('service', None)
        service = service_manager.get_service(service_id)
        if not service:
            return web.json_response({
                "success": False,
                "error": f"Service {service_id} not found"
            }, status=404)
        
        # å¤„ç†æ¶ˆæ¯
        message = data.get('message', '')
        history = data.get('history', [])
        options = data.get('options', {})
        
        # å¤„ç†å›¾åƒ
        if 'images' in data and data['images']:
            message = await process_images(message, data['images'])
        
        # å‘é€æ¶ˆæ¯
        response = await service.send_message(message, history, options)
        
        return web.json_response({
            "success": True,
            "response": response
        })
    except Exception as e:
        logger.error(f"Error in chat endpoint: {e}")
        return web.json_response({
            "success": False,
            "error": str(e)
        }, status=500)
```

#### 4. ä¿å­˜å†å²è®°å½•

```python
@routes.post("/ai/save_history")
async def save_history(request):
    """ä¿å­˜èŠå¤©å†å²è®°å½•"""
    try:
        data = await request.json()
        
        # éªŒè¯æ•°æ®
        if 'history' not in data or not isinstance(data['history'], list):
            return web.json_response({
                "success": False,
                "error": "Invalid history data"
            }, status=400)
        
        # è·å–æœåŠ¡ ID
        service_id = data.get('service', 'default')
        
        # ä¿å­˜å†å²è®°å½•
        history_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "chat_history")
        os.makedirs(history_dir, exist_ok=True)
        
        history_file = os.path.join(history_dir, f"{service_id}_history.json")
        
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(data['history'], f, indent=2)
        
        return web.json_response({"success": True})
    except Exception as e:
        logger.error(f"Error saving history: {e}")
        return web.json_response({
            "success": False,
            "error": str(e)
        }, status=500)
```

#### 5. åŠ è½½å†å²è®°å½•

```python
@routes.get("/ai/history")
async def get_history(request):
    """è·å–èŠå¤©å†å²è®°å½•"""
    try:
        service_id = request.query.get('service', 'default')
        
        history_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "chat_history")
        history_file = os.path.join(history_dir, f"{service_id}_history.json")
        
        if not os.path.exists(history_file):
            return web.json_response({"success": True, "history": []})
        
        with open(history_file, 'r', encoding='utf-8') as f:
            history = json.load(f)
        
        return web.json_response({"success": True, "history": history})
    except Exception as e:
        logger.error(f"Error loading history: {e}")
        return web.json_response({
            "success": False,
            "error": str(e)
        }, status=500)
```

### å¼€å‘æŒ‡å—

#### æ·»åŠ æ–°çš„ AI æœåŠ¡

1. åœ¨ `ai_services/integrations/` ç›®å½•åˆ›å»ºæ–°çš„æœåŠ¡æ–‡ä»¶ï¼Œä¾‹å¦‚ `new_service.py`
2. å®ç° `BaseService` çš„å­ç±»ï¼Œç¡®ä¿å®ç°æ‰€æœ‰å¿…è¦æ–¹æ³•
3. åœ¨ `service_manager.py` ä¸­æ³¨å†Œæ–°æœåŠ¡

```python
# æ–°æœåŠ¡ç¤ºä¾‹
class NewService(BaseService):
    def __init__(self):
        super().__init__(
            service_id="new_service",
            name="New AI Service",
            description="Description of the new service"
        )
    
    async def initialize(self):
        # åˆå§‹åŒ–ä»£ç 
        return True
    
    async def send_message(self, message, history=None, options=None):
        # å‘é€æ¶ˆæ¯å®ç°
        pass
    
    def get_models(self):
        # è·å–æ¨¡å‹åˆ—è¡¨
        return [
            {"id": "model1", "name": "Model 1"},
            {"id": "model2", "name": "Model 2"}
        ]
    
    def get_config_template(self):
        # é…ç½®æ¨¡æ¿
        return {
            "api_key": {"type": "string", "required": True, "description": "API Key"}
        }
```

#### æ·»åŠ æ–°çš„ API ç«¯ç‚¹

1. åœ¨ `server/routes.py` ä¸­å®šä¹‰æ–°çš„è·¯ç”±å¤„ç†å‡½æ•°
2. ä½¿ç”¨ `routes.get()` æˆ– `routes.post()` è£…é¥°å™¨æ³¨å†Œè·¯ç”±

```python
# æ–° API ç«¯ç‚¹ç¤ºä¾‹
@routes.post("/ai/new_endpoint")
async def new_endpoint(request):
    try:
        data = await request.json()
        # å¤„ç†é€»è¾‘
        return web.json_response({"success": True, "data": result})
    except Exception as e:
        logger.error(f"Error in new endpoint: {e}")
        return web.json_response({
            "success": False,
            "error": str(e)
        }, status=500)
```

#### æ•°æ®éªŒè¯

ä½¿ç”¨ `server/validators.py` ä¸­çš„éªŒè¯å‡½æ•°ç¡®ä¿è¯·æ±‚æ•°æ®ç¬¦åˆé¢„æœŸï¼š

```python
def validate_chat_request(data):
    """éªŒè¯èŠå¤©è¯·æ±‚æ•°æ®"""
    if 'message' not in data and 'images' not in data:
        raise ValueError("Request must contain 'message' or 'images'")
    
    if 'options' in data and not isinstance(data['options'], dict):
        raise ValueError("'options' must be an object")
    
    if 'history' in data and not isinstance(data['history'], list):
        raise ValueError("'history' must be an array")
```

### è°ƒè¯•æŠ€å·§

1. æ—¥å¿—è®°å½•
   - é¡¹ç›®ä½¿ç”¨ Python çš„ `logging` æ¨¡å—è®°å½•æ—¥å¿—
   - æ—¥å¿—çº§åˆ«åœ¨ `__init__.py` ä¸­é…ç½®
   - é‡è¦æ“ä½œå’Œé”™è¯¯éƒ½ä¼šè®°å½•åœ¨æ—¥å¿—ä¸­

2. é”™è¯¯å¤„ç†
   - æœåŠ¡å’Œ API ç«¯ç‚¹éƒ½æœ‰å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶
   - é”™è¯¯ä¿¡æ¯ä¼šè¿”å›ç»™å‰ç«¯å¹¶è®°å½•åˆ°æ—¥å¿—

3. æµ‹è¯•å·¥å…·
   - å¯ä»¥ä½¿ç”¨ `curl` æˆ– Postman æµ‹è¯• API ç«¯ç‚¹
   - ç¤ºä¾‹:
     ```bash
     curl -X POST http://localhost:8188/ai/chat \
       -H "Content-Type: application/json" \
       -d '{"service":"g4f","message":"Hello","options":{"model":"gpt_35_turbo"}}'
     ```

## è´¡çŒ®æŒ‡å—

æ¬¢è¿é€šè¿‡ä»¥ä¸‹æ–¹å¼è´¡çŒ®ï¼š

1. æäº¤ bug æŠ¥å‘Šæˆ–åŠŸèƒ½è¯·æ±‚
2. æäº¤ PR ä¿®å¤ bug æˆ–æ·»åŠ æ–°åŠŸèƒ½
3. æ”¹è¿›æ–‡æ¡£

## è®¸å¯è¯

MIT

## ä½œè€…

[Pan Qinjian](https://github.com/panqinjian)

DEFAULT_CONFIG = {
        "service": "g4f",
        "service_list": [
            "G4F(å…è´¹)",
            "é€šä¹‰åƒé—®"
            ],
        "service_ID_list": [
            "q4f",
            "qianwen"
            ],
        "ai_params": {
            "system_prompt": "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œè¯·å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€‚",
            "temperature": 1,
            "max_tokens": 2000,
            "timeout": 300,
            "model": "",
            "proxy": "",
            "api_key": "",
            "api_base": "",
            "api_type": "",
            "api_version": ""
            }
    }

